find / -name context.xml
https://youtu.be/3id6l_BWdNA -- > Ansible END TO END PROJECT In Industry - First Project || Real Time Project
____________________________________

echo $var1                      -- Linux Terminal output

echo "${var1}"				    -- Jenkins/Groovy Terminal output

"This is a variable {{var1}}"   -- Ansible Playbook Terminal output
____________________________________

ansible-playbook gg.yml --syntax-check
____________________________________

usermod -aG docker devopsadmin
# addgroup,docker is a group,devopsadmin is a user
___________________________________
usermod -aG sudo ansibleadmin


WATER FALL MODEL
	The Waterfall Model is a traditional and linear software development methodology.
	The name "waterfall" comes from the idea that each phase of the development process flows steadily downwards,
	just like a waterfall, and one phase must be completed before moving on to the next.

		Requirements Gathering and Analysis
		System Design
		Implementation (Coding)
		Testing
		Deployment
		Maintenance
____________________________________

AGILE MODEL
	Instead of trying to plan and build the entire software project at once, 
	Agile breaks it into smaller, manageable parts called "iterations" or "sprints." 
	Each iteration involves a short development cycle where a small piece of the software is designed, coded, tested, and delivered.

		- It is possible to achive Contineous Intigration
									Contineous Testing
									Contineous Delivery
									
									But NOT Contineous Deployment.
______________________________

DEVOPS PROCESS/STRATERGY

DevOps is a PROCESS  that combine software development (Dev) and IT operations (Ops) to foster collaboration, communication, and automation 
throughout the entire software development and delivery lifecycle.
That includes
			Continuous Development
			Continuous Intigration
			Continuous Testing
			Continuous Delivery
			Continuous Deployment
			Continuous Monitoring
	
	
	Features of DevOps
			Automation- Completely automate the SDLC.
			Collaboration - DEV and Oparation(IT) work togather to make the project productive.
			Integration - automating the build, testing, and deployment processes, enabling frequent and reliable software releases
			Infrastructure as Code - 	Severs are managed and provisioned using code and automation.That improves scalablility and Consistency without any ERRORS.
	
	Benifites of DevOps
			Faster Software Delivery
			Enhanced Quality and Stability
			Improved Collaboration
			Better resource utilization
			scalability and availability.
	
	Stages of Devops
				Planning
				Coding
				Building
				Testing
				Releasing
				Deploying
				Operating
				Monitoring
				
What is meant by build automation?
Build automation is the process of automating the retrieval of source code, compiling it
into binary code, executing automated tests, and publishing it into a shared, centralized
repository. Build automation is critical to successful DevOps processes	

___________________________

Continuous Development 
					==> It is the capability of the Development Team to Continuously develop the Source Code.
Continuous Integration 
					==> It is the process of Continuously Integrating the code for further Testing
Continuous Testing --> 
					==> It is a process of Continuously Testing the changes
Continuous Delivery/Deployment --> 
					==> Both Continuous Delivery/Deployment are used to release the product to Production Environment
	Continuous Delivery  
					==> Will be be promoted to Production Environment with manual Approvals.
	Continuous Deployment 
					==> Starting from Application Build to Production Release - Everything is completely automated, without any downtime. 				
Continuous Monitoring 
					==> MOnitoring the production environments with given tools.


Compile + Uint test + create artifacts
___________________________

Continuous Development
	
Continuous Intigration
	
Continuous Testing
	
Continuous Delivery
	
Continuous Deployment
	
Continuous Monitoring			
___________________________

CI/CD Pipeline


1. Continuous Development:
   - Writing code
   - Code reviews
   - Feature branching and merging
   - Developing and updating documentation
   - Addressing bug reports and issues

2. Continuous Integration:
   - Automatic code compilation
   - Running unit tests
   - Code quality checks (linting)
   - Integration testing
   - Version control system (VCS) integration (e.g., Git)

3. Continuous Testing:
   - Running various levels of tests (unit, integration, system, performance, security)
   - Test case creation and maintenance
   - Automated test execution
   - Regression testing
   - Test environment setup and management

4. Continuous Delivery:
   - Packaging and artifact creation
   - Deployment to staging environments
   - User acceptance testing (UAT)
   - Automated acceptance testing
   - Release candidate preparation

5. Continuous Deployment:
   - Automatic deployment to production environments
   - Canary releases or feature toggles for controlled rollouts
   - Blue-green deployments or rolling updates
   - Monitoring for issues and rollbacks
   - Version control and release tagging

6. Continuous Monitoring:
   - Real-time application and infrastructure monitoring
   - Log analysis and aggregation
   - Performance monitoring and optimization
   - Security monitoring and vulnerability scanning
   - Incident response and alerting

___________________________
Various Cloud Services 
				IAAS
				PAAS
				SAAS
___________________________
	
Deployment Model ::
		
			Public Cloud --> 
			Private Cloud
			Hybrid Cloud  --> Mix of on-premisis and Public or private cloud.
			Community Cloud --> 
___________________________	
		
The AWS Servers are located in different geographical locations!
								Regions
									Availability Zones
										Data centers
											Collection Servers 
													Virtual Machines (EC2 Instances)
___________________________

Git: Version control system for tracking changes in code repositories collaboratively.
Jenkins: Build orchestration Tool OR Automation server for building, testing, and deploying software projects continuously.
Docker: Platform for developing, shipping, and running applications in containers.
Kubernetes (K8s): Container orchestration platform for automating deployment, scaling, and management.
Ansible: Automation tool for configuration management, application deployment, and task orchestration.
Terraform: Infrastructure as Code tool to provision and manage cloud resources programmatically.
Prometheus: Monitoring system for collecting and querying metrics from applications and services.
Grafana: Visualization and monitoring tool for creating dashboards from various data sources.
Maven: Build automation and project management tool for Java-based projects.
___________________________
ssh -i "classkeypair.pem" ubuntu@ec2-100-24-61-71.compute-1.amazonaws.com


Inbound: Data incoming to AWS resources; traffic from external sources to your services.
Outbound: Data leaving AWS resources; traffic from your services to external destinations.
___________________________

Port 22: SSH (Secure Shell) - Used for secure remote access to servers, a fundamental tool for managing and configuring servers in a DevOps context.

Port 80: HTTP (Hypertext Transfer Protocol) - Often used for serving web applications, APIs, and other web-related services.

Port 443: HTTPS (Hypertext Transfer Protocol Secure) - Used for secure communication of web applications and services over SSL/TLS encryption.

Port 8080: Often used as an alternative HTTP port for serving web applications. It's commonly used for development and testing environments.

___________________________                          ___________________________
___________________________ ----AWS DEVOPS TOOLS---- ___________________________

AWS CodePipeline: Continuous integration and delivery service, automating build, test, and deployment pipelines for software development.

AWS CodeBuild: Fully managed build service that compiles source code, runs tests, and produces deployable software artifacts.

AWS CodeDeploy: Automates application deployments to a variety of compute services including EC2, Lambda, and ECS, ensuring consistency and reliability.

AWS CodeCommit: Source control service hosting Git repositories, providing secure and scalable collaboration for software development teams.

AWS CodeArtifact: Package management service for storing, managing, and sharing software packages across development teams securely and efficiently.

=====================================================================================================================================================

add multiple user in user group?
sudo usermod -a -G groupname username

==========================================================================================================
Program VS Scripting 

Program is used for developing a bussiness logic like big packages 
Scripting is used for Automating a manual process.


#!/bin/bash
echo "Please enter the number:"
read num

function prime{
	for((i=2; i<=num/2; i++))
	do
       if [ $((num%i)) -eq 0 ]
       then
          echo "$num is not a prime number."
          exit
       fi
	done
    echo "$num is a prime number."
}
prime

==========================================================================================================
Maven is a build automation tool that helps in install all the dependes/libraries.

Simple Java console application
			New ==> Java Project ==> Create Package ==> Create class ==> Run as Java Application
Java Dynamic Web Application
			New ==> Dynamic Web project ==> Create HTML file in -WebApp- ==>  Run On server ==> Run with file name
Java Maven Console Application
		New ==> Maven project ==> Change -App.java- ==> Run as Java Application
Java Maven Web Application
		New ==> Maven Project ==> Add tomcat dependency in pom.xml ==> Run On server
		
Java SpringBoot application
		New 
==========================================================================================================
Junit or Unit Testing

Checks all the methods are funtioning as expected without errors.
Code Coverage is that how many % of code or the funtions are covered. If less than 100% can't be moved to next phase.

New ==> Java project ==> under java packages create the class java code
					==> under java packages create a new==> Junit test case.


==========================================================================================================
SELENIUM

To perform ****SELENIUM QA**** testing with TestNG(Better reports)
Install TestNG on eclipse 

Create a simple mvn project ==> edit few dependency and plugins https://docs.google.com/document/d/1XnN_W9Ui9jTCW6IXLlaAqLDyPuY-XxH6TsbnORduXGg/edit?usp=sharing ==> Save 
create testclass under src/test/java ==> Write selenium code ==> Save.
Add wedriver ==> create a folder ==> webdriver , add webdriver.exe file (copy paste from local).
mvn build as dry run
Run program as TESTNG ==> Report will be generated ==> Find emailable report.


https://www.saucedemo.com/v1/

=====================================================================================================================================================
MAVEN

Maven sample project (quickstart)
 
 Eclipse ==> New ==> Maven project ==> archetype (smple-quickstart..) ==> create
 Run as ==> a. mvn clean
			b. mvn compile
			c. mvn test
			d. mvn package
			e. mvn install
			f. mvn clean package
			
Maven SpringBoot 
	Springboot Initializer ==> add dependency  spring web & JPA with H2 dependency ==>  Download

	Eclipse ==> Import ==> Exsiting Maven project ==> Dry Run ( Might not work) ==> Under static ,add html file ==> run a maven build ==> Run Java Application
	        ==> open browser localhost/8080 .
			
			
Tomcat > servers.xml
=====================================================================================================================================================

DBMS

Database: A structured repository for efficiently storing, retrieving, and managing data.
Data: Raw information in various forms, valuable when organized and processed.
Database Management System (DBMS): Software enabling database creation, maintenance, and manipulation.
Relational DBMS: A DBMS organizing data into tables with defined relationships, often using SQL for complex queries and data integrity.

1. **Data Query Language (DQL)**:
   - **SELECT**: Used to retrieve data from one or more tables in a database. It is primarily used for querying and fetching data.

2. **Data Definition Language (DDL)**:
   - **CREATE**: Used to create database objects like tables, indexes, and views.
   - **ALTER**: Allows modification of existing database structures, such as adding, modifying, or deleting columns in a table.
   - **DROP**: Deletes database objects like tables, indexes, or views.
   - **RENAME**: Renames an existing database object (e.g., table or column) in some RDBMS systems.
   - **COMMENT**: Adds comments or descriptions to database objects for documentation purposes.
   - **TRUNCATE**: Removes all records from a table while keeping the table structure intact.

3. **Data Manipulation Language (DML)**:
   - **INSERT**: Adds new records into a table.
   - **UPDATE**: Modifies existing records in a table.
   - **DELETE**: Removes records from a table.
   
4. **Data Control Language (DCL)**:
   - **GRANT**: Provides specific privileges (e.g., SELECT, INSERT) to database users.
   - **REVOKE**: Revokes previously granted privileges, restricting user access.

5. **Transaction Control Language (TCL)**:
   - **COMMIT**: Saves all pending transactions to the database.
   - **ROLLBACK**: Reverts changes made during the current transaction to its previous state.
   - **SAVEPOINT**: Sets a point within a transaction to which you can later roll back.

SQL allows users to interact with databases in a structured and efficient manner, making it a crucial tool for managing and querying relational databases.



=====================================================================================================================================================
GIT
___________________________             ___________________________
___________________________ ----GIT---- ___________________________


Version Control System :::
				
		- Used to version control the source code. 
		- Used to track the source code changes.
		

These are common Git commands used in various situations:

start a working area (see also: git help tutorial)
   clone             Clone a repository into a new directory
   init              Create an empty Git repository or reinitialize an existing one

work on the current change (see also: git help everyday)
   add               Add file contents to the index
   mv                Move or rename a file, a directory, or a symlink
   restore           Restore working tree files
   rm                Remove files from the working tree and from the index
   sparse-checkout   Initialize and modify the sparse-checkout

examine the history and state (see also: git help revisions)
   bisect            Use binary search to find the commit that introduced a bug
   diff              Show changes between commits, commit and working tree, etc
   grep              Print lines matching a pattern
   log               Show commit logs
   show              Show various types of objects
   status            Show the working tree status

grow, mark and tweak your common history
   branch            List, create, or delete branches
   commit            Record changes to the repository
   merge             Join two or more development histories together
   rebase            Reapply commits on top of another base tip
   reset             Reset current HEAD to the specified state
   switch            Switch branches
   tag               Create, list, delete or verify a tag object signed with GPG

collaborate (see also: git help workflows)
   fetch             Download objects and refs from another repository
   pull              Fetch from and integrate with another repository or a local branch
   push              Update remote refs along with associated objects

___________________________


The Git workflow typically involves a series of commands to manage and track changes to your codebase. Here's a basic Git workflow with commonly used commands:

1. **Initialize a Repository:**
   - `git init`: Creates a new Git repository in the current directory.

2. **Clone a Repository:**
   - `git clone <repository_url>`: Copies an existing Git repository from a remote location to your local machine.

3. **Stage Changes:**
   - `git add <file>`: Stages a file or changes for the next commit.
   - `git add .` or `git add --all`: Stages all changes in the current directory.

			git config --global user.name "MrNumbrith"
			git config --global user.email "mrnumbrith@gmail.com"

4. **Commit Changes:**
   - `git commit -m "Commit message"`: Saves staged changes with a descriptive message.

5. **View Status:**
   - `git status`: Displays the status of your working directory, showing modified, staged, and untracked files.


6. **Review History:**
   - `git log`: Shows a chronological history of commits.
   - `git log --oneline`: Displays a simplified commit history.
   - git log  --stat   : Extra stat info will be show 
   - git show <commit_id>  : Info of record level
   - git log -2   : To display last 2 commit log.
   - git diff   : Show changes between commits, commit and working tree, etc

7. **Create and Switch Branches:**
   - `git branch <branch_name>`: Creates a new branch.
   - `git checkout <branch_name>`: Switches to an existing branch.
   - `git checkout -b <new_branch_name>`: Creates and switches to a new branch in one command.
   
14. **Branch Management:**
    - `git branch -d <branch_name>`: Deletes a branch (if merged).
    - `git branch -D <branch_name>`: Deletes a branch (regardless of merge status).
	
8. **Merge Branches:**
   - `git merge <TargetBranch_name>`: Combines changes from one branch into another.

9. **Pull from Remote Repository:**
   - `git pull`: Fetches changes from a remote repository and merges them into your current branch.
   
12. **Fetch Remote Changes:**
    - `git fetch`: Retrieves changes from a remote repository without merging.

10. **Push to Remote Repository:**
    - `git push`: Sends your local commits to a remote repository.
	
	- `git remote add origin https://github.com/MrNumbrithGit/FacebookSeleniumTest.git
	- `git remote remove add origin https://github.com/MrNumbrithGit/FacebookSeleniumTest.git	
	- `git push -u origin master
	- 'git remote -v' : List of URLs added
	
11. **Resolve Conflicts:**
    - During merges, if conflicts arise, you need to manually resolve them by editing the affected files.


13. **Undo Changes:** NOT RECOMMENDED TO USE IN SHARED REPO (reset)
		- `git reset <file>`: Unstages changes.
		- `git reset --soft <commit_hash>`: Moves the branch pointer to a previous commit, keeping changes staged.(File in staging)
DEFAULT	-  git reset --mixed <commit_hash>: Moves the branch pointer to a previous commit, unstaging changes and keeping them in your working directory.(File in wrkng dir)
		- `git reset --hard <commit_hash>`: Moves the branch pointer to a previous commit, discarding changes.(File Completely delete)
	Use Case: Useful when you need to rework or discard changes in your working directory or reset the branch pointer.
		- `git revert <commit_hash>`: Creates a new commit that undoes the changes made by a previous commit.
    Use Case: Useful when you need to rework or discard changes in your working directory or reset the branch pointer.

15. **Stash Changes:**
    - `git stash`: Temporarily saves changes without committing.
	-  git stash save "Message" : If you dont give this , it will take old recent commit message.
    - `git stash apply`: Applies the most recent stash.
    - `git stash pop`: Applies the most recent stash and removes it.

16. **Rebase & Squash Commits (Interactive Rebase):**
	-  git rebase -i HEAD~<number_of_commits>: Combines multiple commits into one or more commits interactively during a rebase operation.
											HEAD~6 ==> Interactive mode ==> Type squash for the commit you dont want show ==> comment "#" commit messages.
	- `git rebase <base_branch>`: Moves the current branch to the tip of the `<base_branch>` and reapplies the changes made on the current branch on top of it. 
								  Useful for keeping a linear commit history.
	- `git merge --squash <source_branch>`: Merges changes from `<source_branch>` into the current branch, but squashes all the commits from `<source_branch>` 
										    into a single commit.(Must Commit again)

git merge : This gives us the complete picture of the commit history and branch evolution.
git rebase : This will cleans up commit on that branch by moving commit history to the tip of the main branch.
git squash : This will clean up all the commit making that into a single commit.


git rm <file(s)> : Removes the specified file(s) from both the working directory and the Git index.



Git MERGE vs REBASE vs SQUASH
https://youtu.be/0chZFIZLR_0?si=RWXEQz2LPbXms_cn



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     --> TOKEN


ERRORS FACED
git remote on Ec2 instance.
workflow ON > GitHub PAT token settings
git remote set-url origin https://<username>:<new-personal-access-token>@github.com/<username>/<repository>.git
git remote set-url origin https://MrNumbrithGit:<personal-access-token>@github.com/MrNumbrithGit/JavaGitest.git


---------------------------------------
Branching Stratergies

Master_Branch
	Release_Branch
		Intregration_Branch
			Feature1
			Feature2

#Merging will be down to up 

---------------------------------------
clone the repo --> Make changes in new branch --> push --> On github --> Compare and pull --> merge.

==========================================================================================================
ADDRESS BOOK APP

Create EC2 ==> Installed git java maven tomact ==> cloned the code (addressBook)
      apt-get update
      java --version
      apt install java
      java --version
      apt install default-jre
      apt install maven
      java --version
      mvn --version
      systemctl start tomcat9
      systemctl status tomcat9
	  git clone https://github.com/vikulrepo/Addressbook2.git
	  mvn clean package
      systemctl start tomcat9
      systemctl status tomcat9
	  
	  mv /home/ubuntu/Addressbook2/target/addressbook.war /var/lib/tomcat9/webapps
	  
	  http://<public_ip>/addressbook/
	

Created ==> Maven quickstart ==> pom.xml added dependencies ==> Test.java code added ==> Driver added ==> 	mvn compile ==> exported (as runnable jar)
==> used command (java -jar fbtest.jar)


==========================================================================================================

MAVEN DEPENDENCIES 

https://mvnrepository.com/artifact/org.seleniumhq.selenium/selenium-java/4.11.0

<!-- https://mvnrepository.com/artifact/org.seleniumhq.selenium/selenium-java -->
<dependency>
    <groupId>org.seleniumhq.selenium</groupId>
    <artifactId>selenium-java</artifactId>
    <version>4.11.0</version>
</dependency>

https://mvnrepository.com/artifact/org.apache.maven.surefire/surefire/3.1.2

<!-- https://mvnrepository.com/artifact/org.apache.maven.surefire/surefire -->
<dependency>
    <groupId>org.apache.maven.surefire</groupId>
    <artifactId>surefire</artifactId>
    <version>3.1.2</version>
    <type>pom</type>
</dependency>

=====================================================================================================================================================

JENKINS 
		- The build orchestration ###Jenkins is one of the open source build orchestration tool###
		Pre-requisites :JM--> JDK,JENKINS , GIT TOOL config
						JS--> JDK, MAVEN , DOCKER , GIT

Default loc  -->  var/lib/jenkins

			Developer's Perspective
				- are just consumers.
						
			DevOps Perspective
			
				Administrator :::
				
				- Installation of jenkins 
				- Manage the plugins 
				- User Management 
				- Credential Management
				- Tools Management 
				- Jenkins Master/Slave - Node Configuration and Management
				- Security Management 
				- Backup & Recovery 
				- Upgrade
				- Create jenkins CI/CD Pipelines Project to onboard application
				- Troubleshooting
				
				
			Jenkins Architecture :::
				
				Jenkins Master / Slave Architecture :::
				
				Jenkins_Master 				VM 	==> To create Jenkins CI/CD Pipeline Jobs & Schedule the jobs
					Jenkins_Slave-Node1		VM	==> Java Appln. Build ==> git, jdk, maven	# Application Build Server
					Slave-Node2				VM	==> Python Appln. Build ==> git, python	# Application Build Server
					Slave-Node3				VM	==> C# Appln. Build ==> git, jdk, maven	# Application Build Server

			
	

Configuration of Slave Node 
Jenkins ==> Manage Jenkins ==> Node ==> Name , PermanentUser ==> Labels (MustName) , Number of executors (No of Build Simultaniounly) 
==> Remote root directory (Where my workspace should be created) ==> Host (Private IP address) ==> Add Credentials (Kind:SSH Username with Pvt Key) ==> ID & Discrip(Can be your choise)
==> Add Credentials ==> Host Key Verification Strategy (Manually trusted key verification stratergy) ==> Require manual verification of initial connection (Tick)
==> Save ............ ==> Still unsuccessfull ==> Go to the Node ==> Trust SSH host key (Yes).


Under FreeStyle if you want run builds on NODES , (Restrict where this project can be run) Give Name of the Node.
/home/devopsadmin/workspace/Q     This dir will be created in Slave/Node Machine.



SOURSE ==
		/home/devopsadmin/workspace/PipelineOne/target/mvn-hello-world.war

DESTINATION ==
		/var/lib/tomcat9/webapps
		
		
SSH PUBLISHER		
SLAVE/ WORKSPACE ----Share files----> QA SERVER

	1.QA Server needs java and tomcat for project to run ,INSTALL.
	2.Create a user , ssh key
	3.chown -R devopsadmin /var/lib/tomcat9/webapps     From Root
	4.To ssh using Jenkins , Plugin: publish over ssh.
5.Manage Jenkins > System > Fill under Publish over SSH> SSH Servers (Name, Hostname[Pvt IP], username,Remote Directory (/var/lib/tomcat9/webapps)) > Advanced > Key > Success.
	6.Configure groovy Script, Project > Configure > Pipeline Syntax > Sample Step (ssh publisher) > SSH Server Name (QA_Server) > 
		Source file (target/mvn-hello-world.war) > Remove prefix (target/) > Remote directory (.)(What is in configuration(So its Dot)) > Generate Pipeline Script
	7.Edit Script added the snippet under script{}
	8.Build > Check > IP:8080/mvn-hello-world

===
devopsadmin
devopsadmin ALL=(ALL) NOPASSWD: ALL
13.235.71.116
sshPublisher(publishers: [sshPublisherDesc(configName: 'Kubernetes', transfers: [sshTransfer(cleanRemote: false, excludes: '', execCommand: 'kubectl apply -f k8smvndeployment.yaml', execTimeout: 120000, flatten: false, makeEmptyDirs: false, noDefaultExcludes: false, patternSeparator: '[, ]+', remoteDirectory: '.', remoteDirectorySDF: false, removePrefix: '', sourceFiles: '*.yaml')], usePromotionTimestamp: false, useWorkspaceInPromotion: false, verbose: false)])
====
Build Triggers
		
- Build Periodic
	Is used to schedule the job based on cron tab.
	MINUTE HOUR DOM MONTH DOW
	*/2 * * * *   >  At every 2nd minute
	*/15 * * * *  > At every 15th minute
					
- github webhook
	It is used to trigger the jenkins job, whenever there is any changes happened in the src_code.
	Create- Web hook configuration in github repo.				
		> GitHub > 	Repo > Settings > Webhooks > Payload URL :	http://54.166.164.210:8080/github-webhook/	> Content type (applicatio/json) 
		> event type (Just the push event.) > Add webhook. >  
			
			
- Poll SCM  (Only if there is changes)
	It is used to trigger the jenkins job, based on the schedule and if there is any changes happened in the src_code.
	Define  Cron Tab.
	MINUTE HOUR DOM MONTH DOW
	*/2 * * * *   >  At every 2nd minute
	*/15 * * * *  > At every 15th minute
	
Email Notification Plugins
Manage Jenkins	> System > E-mail Notification > Follow below steps
	SMTP Server : smtp.gmail.com
	@gmail.com
	
	SMTP Authentication (Tick)
	Use SSL (Tick)
	SMTP Port :: 465	
	UserName:mrnumbrith@gmail
	password :dpar qghn qkuj ofnt
	
-----------------------------------
Create a JENKINS pipelines job > run shedule every 4 hours > to clean up the workspace directory in slave machine
-----------------------------------

pipeline {
    agent { label 'JavaSlave' }

    stages {
        stage('CleanUp') {
            steps {
				sh 'cd /home/devopsadmin/'
				sh 'rm -rf workspace/*'
            }
        }
    }
}
-----------------------------------
Docker LogIn Password (Token) On Jenkins

Manage Jenkins > Credentials > Add Credentials > Kind : username with password > Username : mrnumbrithdocker > Password : DockerToken > ID & Discrip : Docker pass > Create


===============================================
pipeline {
    agent { label 'javaslave1' }
    
    tools {
        maven "maven_3.6.3"
    }
    
    stages {
        stage('SCM_Checkout') {
            steps {
                echo 'SCM Checkout'
				git 'https://github.com/LoksaiETA/Java-mvn-app2.git'      #This step fetches the source code from the repository and places it in the Jenkins workspace so that subsequent stages can work with it.
            }
        }
        stage('App_Build') {
            steps {
                echo 'Perform Maven Build'
				sh 'mvn -Dmaven.test.failure.ignore=true clean package'
            }
        }
        stage('Deploy to QA Environment') {
            steps {
				script {
					sshPublisher(publishers: [sshPublisherDesc(configName: 'QA-Server', transfers: [sshTransfer(cleanRemote: false, excludes: '', execCommand: '', execTimeout: 120000, flatten: false, makeEmptyDirs: false, noDefaultExcludes: false, patternSeparator: '[, ]+', remoteDirectory: '.', remoteDirectorySDF: false, removePrefix: 'target/', sourceFiles: 'target/mvn-hello-world.war')], usePromotionTimestamp: false, useWorkspaceInPromotion: false, verbose: false)])
				}
            }
        }
    }
}

===============================================
FreeStyle
-------
pipeline
SCM_Checkout
App_Build > mvn clean package
Deploy to QA Environment


Build Triggers	
- Build Periodic	
- github webhook
- Poll SCM 

Email Notification Plugins
===============================================
pipeline {
    agent any
    
    stages {
        stage('Checkout') {
            steps {
                git 'url'
            }
        }
        
    }
}


pipeline
	agent ?
	
	stages
		stage
			steps
			 {}
		stage
			steps
			 {}
		stage
			steps
			 {}
			 
			
			
			
FreeStyle_git_maven_tomcat > Configure > Source Code Management > Git_Repositories Credentials none > Branches to build ,Master > 

=====================================
TOMCAT 

sudo apt install openjdk-11-jre

#Tar file(Download link)
https://dlcdn.apache.org/tomcat/tomcat-9/v9.0.80/bin/apache-tomcat-9.0.80.tar.gz

java --version

#Pull tar file
wget https://dlcdn.apache.org/tomcat/tomcat-9/v9.0.80/bin/apache-tomcat-9.0.80.tar.gz

#Upzip tar file
tar -xvzf apache-tomcat-9.0.80.tar.gz

#tar is no more required
rm apache-tomcat-9.0.80.tar.gz


/home/ubuntu/tomcat9/bin

sh startup.sh

Check 
IP:8080


#you'll need to edit the Manager's context.xml file.

vi /home/ubuntu/tomcat9/webapps/host-manager/META-INF/context.xml
vi /home/ubuntu/tomcat9/webapps/manager/META-INF/context.xml

#COMMENT
  <!--  <Valve className="org.apache.catalina.valves.RemoteAddrValve"
  allow="127\.\d+\.\d+\.\d+|::1|0:0:0:0:0:0:0:1" /> -->
  
#Add role and uname passwd

/home/ubuntu/tomcat9/conf
vi tomcat-users.xml

    <role rolename="manager-gui"/>
	<role rolename="admin-gui"/>
    <role rolename="manager-script"/>
	<role rolename="admin-script"/>
    <role rolename="manager-jmx"/>
    <role rolename="manager-status"/>
    <user username="admin" password="admin" roles="manager-gui,manager-script,admin-gui,admin-script,manager-jmx,manager-status"/>


#Try loggingIN


On Jenkins 
Post Build action 
	- Deploy as container
	- **/*.war
	- Credentials  (admin , admin)
	- Tomcat URL  (http://54.165.20.140:8080/)
	
	
<tomcat-users>
    <role rolename="manager-gui"/>
    <role rolename="admin-gui"/>
    <role rolename="manager-script"/>
    <role rolename="admin-script"/>
    <role rolename="manager-jmx"/>
    <role rolename="manager-status"/>
    <user username="admin" password="admin" roles="manager-gui,manager-script,admin-gui,admin-script,manager-jmx,manager-status"/>
</tomcat-users>

=====================================================================================================================================================

DOCKER
	A Docker container encapsulates a single application. Sometimes, it can encapsulate multiple apps. 
	But standard procedure is to have a single application running in a container.
	A virtual machine encapsulates an entire operating system.



				Terminologies :::
				
					Containerization :
						It is the process of packaging the application along with its dependencies.
					
					Namespace (NS)			--> Used to create independent process to the containers
					Control Groups (CG)		--> Used to create the level of isolation to run the containers in the namespace.
					
					Container Engine 		--> Used to create the container Images, Run the Container, Manage the Container resources.
												It acts as an interface between the underlying OS and the Containers.
					Image					--> Images are the static file,
												Image are Non-Executable.
					Container				--> Containers are Executable unit of Image. 
					Container Registry		--> It is used to Store and Manage the Container Images.
												dockerhub - is the default container registry for Container Images.
												www.hub.docker
-----------------------------------
Docker Swarm 
		Is the Container Orchestration Tool used to Orchestrate only the Docker Containers.	Ensure High Availability of Container.		
Docker Compose 	
		It is used to run multiple Containers as a Service.												
-----------------------------------
#build from a docker file. (.)
docker build -t mrnumbrithdocker/demonew:0.2 .
docker build -t smitachawla67/hello:1.8 .
#Docker Login
docker login -u mrnumbrithdocker

#docker push imagename
docker push mrnumbrithdocker/demonew:0.2

#tO Run the appli on -itd with -p
docker run -itd -p 7070:8080 --name DemoNewwar smitachawla67/demonew:1.8

#Create and run a container from an image, with a custom name:
docker run --name <container_name> <image_name>

Run a container with and publish a containerâ€™s port(s) to the host.
docker run -p <host_port>:<container_port> <image_name>

Run a container in the background
docker run -d <image_name>

Start or stop an existing container:
docker start|stop <container_name> (or <container-id>)

Remove a stopped container:
docker rm <container_name>

Open a shell inside a running container:

<container_name> /bin/bash

Fetch and follow the logs of a container:
docker logs -f <container_name>

To inspect (Detailed list) a running container:
docker inspect <container_name> (or <container_id>)

To list currently running containers:
docker ps

List all docker containers (running and stopped):
docker ps --all

View resource usage stats
docker container stats

View hostory of a container
docker history <container_name> (or <container_id>)

-----------------------------------

for linux in aws docker basics run all the commands present there


docker info-gives you the infomation about the docker
docker images -images the already exist
docker ps  - Running container , how many containers are they or details of container or running conatiner
docker ps -a -all containers
docker pull osname(ubuntu)-download image
docker run imageName    : To run in foreground(attached mode) , If there is no task to run it will exit.
dockers run -it(interactive)   /  dockers run -d(dettached mode , run it as a background process) osname(ubuntu)    :it will give you  large number of charcaters(to create a new container)
docker exec -it containerid bash -to enter inside the container
exit -to come out of the container and then commit
docker commit containername(docker hub name)/jsptest1 -create an images out of a container (which is modified by you and then you want to save the container)
docker login - to login into your dockerhub(there you want to enter your docker hub username and password)
docker push ImageName(dockerhub name/jsptest1) -moves or store the container into your docker hub
docker tag source(jsptest2) destination(dockerhubname/jsptest2)-creating a copy of container
apt-get install apache2- installing apache
service apache2 start -starting the apache
ps -ef -listing the services
ps -ef | grep apache2- where the word apache2 is they are displayed
ps -ef | wc -l -how many lines are present
cat ports.conf -the port number of s/w
we can't change port or mapping after starting so we need to do it before starting service
docker run -it -p(publish) 80 80(port of base os  and guest binaries) -d osname(ubuntu)-to mapping  
docker run -it -p 8080:8080 tomcat
-----------------------------------
Container Volumes are used to run any stateful Application and maintain persistant volumes to permanently store the data generated by the application inside the container.
			
			docker volume list
			
			docker volume create vol_one 
			
			docker volume inspect vol_one
			
			docker run -it --mount source=vol_one,destination=/vol_one tomcat /bin/bash
			
			To send any input files to container or to access teh output files from container, we use container volume.
-----------------------------------
mrnumbrithdocker
offset-->12345
	
docker login -u mrnumbrithdocker
dckr_pat_MNofb-uKGD1qOYTr5PYJD_C-HyM


Manage Jenkins > Credentials > Add Credentials > Kind : username with password > Username : mrnumbrithdocker > Password : DockerToken > ID & Discrip : Docker pass > Create
-----------------------------------
				Instructions ::
				
				FROM  	--> Used to specify the base image. 
				RUN 	--> Used to run the package managers to manage the package within the container.			
				COPY	--> Used to copy the files from host machine into the container volume/Dir. 		
				CP		--> Used to copy the files with the container volumes/dir.	
				WORKDIR --> Used to identify the current working directory.	
				ENV 	--> Used to Create Environment Variables	
				EXPOSE	--> It is used to Expose the Container Port. 
				CMD		--> Used to specific the default/start-up command to run the container. This allow users to pass any commands at runtime. 
				ENTRYPOINT	--> Used to specific the default/start-up command to run the container.This Will not allow users to pass any commands at runtime.
-----------------------------------							
Dockerfile

FROM tomcat:9.0
COPY target/*.war /usr/local/tomcat/webapps/
EXPOSE 8080
CMD ["catalina.sh", "run"]

-----------------------------------
# Use an appropriate base image with Java installed
FROM openjdk:11
# Set the working directory inside the container
WORKDIR /app
# Copy the JAR file into the container
COPY springboot.jar /app
# Specify the command to run when the container starts
CMD ["java", "-jar", "springboot.jar"]


docker build -t vineethkvr1/rock:0.1 .

docker run -itd -p 5050:8080 --name DemoNewjar vineethkvr1/rock:0.1

744f2b2f2a05

docker exec -it 744f2b2f2a05 /bin/bash
-----------------------------------



Jenkins 
					  SCM_Checkout	--> Application_Build(*.war)	--> Docker_Build(App_Img)	--> Publish_to_Docker_Resigtry 
					  
Servers :
	Jenkins_Master						==> git,jdk,jenkins 		
										==> It is create the build pipeline & Schedule the job to node.
		Java_Appln_Build_Server			==> git,jdk,maven,docker	
										==> It is used to build the Application and build Container image
										==> Push the images to Container Registry.					  

- Jenkins_Master & Node Configuration is successful
		Maven tool configuration
		usermod -aG docker devopsadmin
		User_ID in build server with SSH Keys
		Config the Node in Jenkins_Master
							
- Access to Dockerhub from Jenkins.
		Configure the Dockhub credentials in Jenkins_Master
		Create Access Token in Dockerhub
		Login to Dockerhub
		Publish_to_Docker_Registry				
							admin
-----------------------------------

pipeline {

    agent { label 'slave1' }
	
    tools {
        maven "maven-3.6.3"
    }

	environment {	
		DOCKERHUB_CREDENTIALS=credentials('dockerloginid')
	}
	
    stages {
        stage('SCM_Checkout') {
            steps {
                echo 'Perform SCM Checkout'
				git 'https://github.com/LoksaiETA/devops-java-webapp.git'
            }
        }
        stage('Application_Build') {
            steps {
                echo 'Perform Maven Build'
				sh 'mvn -Dmaven.test.failure.ignore=true clean package'
            }
        }
        stage('Build Docker Image') {
            steps {
				sh 'docker version'
				sh "docker build -t loksaieta/loksai-eta-app:${BUILD_NUMBER} ."
				sh 'docker image list'
				sh "docker tag loksaieta/loksai-eta-app:${BUILD_NUMBER} loksaieta/loksai-eta-app:latest"
            }
        }
		stage('Login2DockerHub') 

			steps {
				sh 'echo $DOCKERHUB_CREDENTIALS_PSW | docker login -u $DOCKERHUB_CREDENTIALS_USR --password-stdin'
			}
		}
		stage('Publish_to_Docker_Registry') {
			steps {
				sh "docker push loksaieta/loksai-eta-app:latest"
			}
		}
    }
}	

-----------------------------------
Docker Compose 	
		It is used to run multiple Containers as a Service.
		
		
STEPS
		apt install docker-compose
		vi docker-compose.yml
		docker-compose up
		docker-compose up -d
		check ip:8080
		
		
		




 Docker Compose file   (yaml)
 
version: '3'  # Compose yml file version (e.g., '3', '3.8')

services:
  webapp:    # Service name
    image: nginx:latest  # Docker image to use for the service
    ports:
      - "80:80"  # Port mapping (host_port:container_port)
    volumes:
      - ./webapp:/usr/share/nginx/html  # Mount a host directory as a volume
    environment:
      - NGINX_ENV=production  # Environment variables

  database:
    image: mysql:5.7
    environment:
      - MYSQL_ROOT_PASSWORD=mysecretpassword
      - MYSQL_DATABASE=mydb

networks:
  mynetwork:  # Custom bridge network
    driver: bridge

volumes:
  myvolume:  # Named volume





=====================================================
alias di='docker images'
alias dps='docker ps -a'
alias c='clear'

docker run -itd -p 8080:8080 --name addressbookCon mrnumbrithdocker/addressbookvikul:0.1 

=====================================================================================================================================================


KUBERNETES




			Terminologies :
				Images 
				Containers 
				Container Registry 
				Repositories
				
				Kubernetes_Master		==> Is a controller Machine, shedules the deployments 
				Kubernetes_Cluster 		==> Is a collection of Kubernetes_Worknodes
				Kubernetes_Worknodes 	==> Are the nodes where the actual deployment happens.
				Kubeadm					==> Is the command line utility  used to initiate the Kubernetes Master & its resource.
				kubectl					==> It is a command line utility used to interact with Kubernetes master.
				
				PODs					==> Atomic Unit of Scheduling.
				
				
Pod Communication is done through (Auto generated IP)
Inter - Ip
Intra - Ip:port

Kubernetes architecture is designed to manage containerized applications across a cluster of machines efficiently and at scale. Here's an overview of the key components and concepts in Kubernetes architecture:

1. **Master Node**:
   - The master node is the control plane of the Kubernetes cluster. It manages and coordinates all activities within the cluster. Key components on the master node include:
     - **API Server**: The central management point that exposes the Kubernetes API, which is used to interact with the cluster.
     - **etcd**: A distributed key-value store that stores the cluster's configuration data, such as the desired state of resources.
     - **Controller Manager**: Manages controller processes responsible for maintaining the desired state of resources, such as deployments, replicasets, and services.
     - **Scheduler**: Assigns work to worker nodes based on resource requirements, constraints, and other policies.

2. **Worker Node**:
   - Worker nodes, also known as worker or minion nodes, are the machines that run containerized applications. Each worker node hosts multiple pods, which are the smallest deployable units in Kubernetes. Key components on a worker node include:
     - **Kubelet**: Ensures that containers are running in a Pod by communicating with the container runtime (e.g., Docker) and reporting the pod's status to the control plane.
     - **Container Runtime**: The software responsible for running containers (e.g., Docker, containerd).
     - **Kube Proxy**: Manages network communication between pods and services, including load balancing and network policies.

3. **Kubeconfig**: Configuration files used by administrators, developers, and tools to interact with the Kubernetes cluster. It includes details like the API server's address and authentication credentials.

4. **kubectl**: A command-line tool that allows users to interact with the Kubernetes cluster. It uses the kubeconfig file to authenticate and communicate with the API server.

5. **Pod**: The smallest deployable unit in Kubernetes. It can contain one or more containers that share the same network namespace and storage volumes. Pods are scheduled to run on worker nodes.

6. **ReplicaSet and Deployment**: Controllers that manage the desired number of replica pods and ensure that the desired state is maintained, even if pods fail or are terminated.

7. **Service**: An abstraction that provides a stable network endpoint to access a set of pods. Services can be used for load balancing, service discovery, and routing traffic to pods.

8. **Namespace**: A way to logically divide a Kubernetes cluster into multiple virtual clusters, allowing for isolation and resource allocation. Namespaces are used to organize and separate resources.

9. **ConfigMap and Secret**: Kubernetes resources for managing configuration data and sensitive information (such as passwords and API keys), which can be used by containers and pods.

10. **Ingress**: Manages external access to services, typically HTTP. It provides routing rules, load balancing, and SSL termination for incoming traffic.

11. **Volume**: A directory that may be mounted into a pod's containers. It can be used for persisting data or sharing files between containers.

12. **Persistent Volumes (PVs) and Persistent Volume Claims (PVCs)**: Resources that manage storage in a cluster. PVCs are used by pods to request storage, while PVs are the physical storage resources that are provisioned and managed by administrators.

13. **StatefulSet**: A controller for deploying and managing stateful applications, such as databases, where each pod has a stable and unique identity.

14. **Custom Resource Definitions (CRDs)**: Extends the Kubernetes API with custom resources and controllers, enabling the creation of custom resource types and operators.

Kubernetes architecture provides a highly scalable, resilient, and flexible platform for deploying, managing, and scaling containerized applications. It abstracts the underlying infrastructure and automates many operational tasks, making it easier to run cloud-native applications in production.


K8s Cluster Pre-requisites 
									
	Launch VMs - Host_Machines -- KM,(KWN1,2,3,...)
		3GB RAM 
		2 CPUs 
	K-Master&K-Node
		All the Nodes should have full network assess. 
		There should not be any firewall restriction. 
		All the required ports should be enabled.
																
	K-Master&K-Node
		To be install in all the nodes - Master & Workernodes.
		Setup Host_Machine Names -- KM,(KWN1,2,3,...)
		Docker 
		ContainerD ==> CRI - Container Runtime Interface 
		kubeadm,kubectl,kubelet
		Ports To be enabled
	K-Master								
		To be executed only in Master
		kubeadm init to initialize Kubernetes master 
	K-Node					
		To be executed only in WorkerNodes
		kubeadm join to join the workernodes kubernetes Master
		
		
| Aspect                   | Pods                                                              | Deployments                            |
|--------------------------|-------------------------------------------------------------------|----------------------------------------|
| Primary Purpose          | Single-instance application containers                            | Managing replica sets for applications  |
| Scaling                  | Typically not used for scaling                                    | Used for scaling and managing replicas  |
| Maintaining State        | Pods are ephemeral and can be easily replaced or deleted          | Deployments ensure the desired number of replicas are running and maintain application availability |
| Updates/Rollbacks        | No built-in support for updates or rollbacks                      | Support rolling updates and rollbacks to ensure zero-downtime updates |
| Selectors                | May not have label selectors, making it harder to group and manage| Use label selectors to group and manage pods, allowing easy scaling and updates |
		
=======================================================

On Restart deployment,ID will change


				Services :::
					ClusterIP 	(Default Service)
					NodePort 	- It is used to expose the pods to internet.
					Load Balancer		

					
			Kubernetes Namespaces 			==> Logical Partitioning of the Cluster.
					Can be created based on the Application / Environments.
					
=======================================================

Pod Creation
		nginx.yml (file) > kubectl create -f nginx-pod.yaml  > kubectl get pod > # (To get inside the pod )  kubectl exec -it nginx-pod -- /bin/sh >
		Create test HTML page > (Expose PODS using NodePort service ) kubectl expose pod nginx-pod --type=NodePort --port=80  > 
		kubectl get svc (Node port /44545)  >  Open Web-browser and access webapge using IP:NODE_PORT 
		

OverView
		Deployment -> Pod --> Container --> Application
					->If pod fails ,New Pod will resume the task(Self-healing)

		NodePort Service ==> It is used to expose the pod to internet.
		Using the External IP Address of the Node and the NodePort, we can access the application that is running inside the pod thru web browser.
		


# Create and display PODs
kubectl create -f nginx-pod.yaml
kubectl get pod
kubectl get pod -o wide
kubectl get pod nginx-pod -o yaml
kubectl describe pod nginx-pod


=======================================================
Deployment Creation

Deployment Controller Object  high - availability of pods, autoscale, upgrade, rollback(undo) the changes.

"nginx-deploy-xxxxx-yyyyy," where "xxxxx" is a Deployment-specific identifier, and "yyyyy" is a unique identifier for the ReplicaSet.




=======================================================
# 2. Create and Display Deployment

kubectl create -f nginx-deploy.yaml 
kubectl get deploy -l app=nginx-app
kubectl get rs -l app=nginx-app
kubectl get po -l app=nginx-app
kubectl describe deploy nginx-deploy

*******************************************************************
# 3. Testing: Rollback update 

kubectl set image deploy nginx-deploy nginx-container=nginx:1.91 --record
kubectl rollout status deployment/nginx-deploy
kubectl rollout history deployment/nginx-deploy
kubectl rollout undo deployment/nginx-deploy
kubectl rollout status deployment/nginx-deploy
kubectl describe deploy nginx-deploy | grep -i image

*******************************************************************
# 4. Testing: Update Version of "nginx:1.7.9"  to "nginx:1.9.1"

kubectl set image deploy nginx-deploy nginx-container=nginx:1.9.1
# or
kubectl edit deploy nginx-deploy
kubectl rollout status deployment/nginx-deploy
kubectl get deploy

*******************************************************************
# 5. Testing: Scale UP

kubectl scale deployment nginx-deploy --replicas=5
kubectl get deploy
kubectl get po -o wide

*******************************************************************
# 6. Testing: Scale DOWN

kubectl scale deployment nginx-deploy --replicas=3
kubectl get deploy
kubectl get po -o wid

=======================================================


kubectl set image pod tomcat-demonew tomcat-demonew-container=mrnumbrithdocker/demonew:0.1



alias kgs='kubectl get svc'
alias kgp='kubectl get pods'
alias kgd='kubectl get deploy'
alias kgn='kubectl get ns'

alias krrd='kubectl rollout restart deploy'
alias kcuc='kubectl config use-context'
alias c='clear'
alias kdp='kubectl describe pod'
alias ge=' grep -i error --colour'
alias kl='kubectl logs'
alias k='kubectl'kl 
alias g='grep'
alias s0='kubectl scale --replicas=0 deployment'
alias s1='kubectl scale --replicas=1 deployment'		



===============================================
docker search mrnumbrithdocker

kubectl get all

kubectl create -f nginx-deploy.yaml 

kubectl exec -it deployment/<deployment-name> -- /bin/bash

kubectl expose deployment tomcat-deploy --type=NodePort --port=8080

kubectl set image deploy tomcat-deploy tomcat-container=mrnumbrithdocker/demonew:0.2

kubectl rollout undo deployment/tomcat-deploy

kubectl describe pod tomcat-deploy | grep events

kubectl rollout history deployment/nginx-deploy

kubectl scale --replicas=10 deployment tomcat-deploy-addbook

kubectl delete -f nginx-deploy.yaml
------------------------------
kubectl set image deploy springweb spring-container=springweb:1.2
kubectl rollout undo deployment/springweb

kubectl delete -f deploy-tom.yml



------------------------------
ISSUE FACED

STUCK IN TERMINATION  / PENDIND
tomcat-deploy-addbook-6689bc547d-2xkg5   1/1     Terminating   0          5m9s
tomcat-deploy-addbook-6689bc547d-46jvz   1/1     Terminating   0          5m9s
tomcat-deploy-addbook-6689bc547d-4zztj   1/1     Terminating   0          5m9s

tomcat-deploy-addbook-6689bc547d-n7z6l   0/1     Pending       0          29s

kubectl get nodes

#On Node
sudo systemctl restart kubelet
sudo systemctl status kubelet

 

=====================================================================================================================================================

ANSIBLE

				Ansible is IAC Tool used for Configuration Management.
					It is simple and Agentless
					It is powerful IAC Tool
					It uses push mechanism to manage the target machines.
				
				
				Ansible is used to :
					- Configure the Server - Installation/Unstallation/Upgrade of tools/Services
					- Deployment of Applications 
					- Used to perform Security Patch updates in all the target Machines
					
					- Should be reusable
					

		

===========================================		
[testnodes]
samplenode1 ansible_ssh_host=54.157.192.63 ansible_ssh_user=ansibleadmin
samplenode2 ansible_ssh_host=172.31.37.129 ansible_ssh_user=ansibleadmin


ansible <hosts_name> -m <module_name> -i <inventory_file>

ansible testnodes -m ping

ansible dev_server_grp1 -m ping -i dev_servers

ansible devnodes -m ping -i /etc/ansible/dev_inventory

#host machines can be identified using :
#all | group_name | individual_host_name

===========================================

Ansible AD-HOC commands

ansible -i my_inventory.ini -m shell -a 'df -h' my_servers

In this example:

- `-i my_inventory.ini`: Specifies a custom inventory file named `my_inventory.ini`.

- `-m shell`: Uses the `shell` module to execute a shell command on remote hosts.

- `-a 'df -h'`: Passes the `df -h` command as an argument to the `shell` module.

- `my_servers`: The host pattern that selects the hosts or group of hosts named `my_servers` from the inventory.


- 'all'   for all the nodes
This command will execute the `df -h` command on all hosts listed under the `my_servers` group in the custom inventory file.


===========================================
su - devopsadmin
/etc/ansible

---
- hosts: target-instance
  become: yes
  tasks:
    - name: Install Git
      apt:
        name: git
        state: present

    - name: Install Maven
      apt:
        name: maven
        state: present

    - name: Install Tomcat9
      apt:
        name: tomcat9
        state: present



===========================================
echo $var1                      -- Linux Terminal output

echo "${var1}"				    -- Jenkins/Groovy Terminal output

"This is a variable {{var1}}"   -- Ansible Playbook Terminal output

==========================================

Playbook Reusablelity
Variables
Variables file
Dynamic Variables
Package manager Playbook
Handler
Notify 



---
- hosts: "{{ host_name }}"
  become: yes
  tasks:
    - name: Managing "{{ tool_name }}" service
      "{{ module_name }}":
        name: "{{ tool_name }}"
        state: "{{ tool_state }}"




#  ansible-playbook apt_update.yaml -e "host_name=testnodes tool_name=nginx module_name=apt tool_state=present"

#  state: present / absent / latest

#  ansible-playbook gg.yml --syntax-check


	   
	   
	    
sudo usermod -aG sudo ansibleadmin


---
- hosts: all # Replace with the actual target hosts or host group
  become: yes  # If file destination requires superuser privileges

  tasks:
    - name: Copy a file to remote hosts
      copy:
        src: /etc/ansible/playbook_area/file.txt  # Path to the source file on the local machine
        dest: /home/ansibleadmin/ansi    # Destination path on the remote machine




ansible all -m copy -a "src=/etc/ansible/playbook_area/file.txt dest=/home/ansibleadmin/ansi"

=====================================
ROLES
Ansible Roles are a way to organize and structure your Ansible playbooks to promote reusability and maintainability. 

my_role/
â”œâ”€â”€ defaults/
â”‚   â””â”€â”€ main.yml
â”œâ”€â”€ files/
â”œâ”€â”€ handlers/
â”‚   â””â”€â”€ main.yml
â”œâ”€â”€ meta/
â”‚   â””â”€â”€ main.yml
â”œâ”€â”€ tasks/
â”‚   â””â”€â”€ main.yml
â”œâ”€â”€ templates/
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ inventory
â”‚   â””â”€â”€ test.yml
â”œâ”€â”€ vars/
â”‚   â””â”€â”€ main.yml
â””â”€â”€ README.md
```

- **defaults**: Default variables for the role.
- **files**: Static files to be copied to the target hosts.
- **handlers**: Event-driven tasks that are triggered by other tasks.
- **meta**: Metadata for the role, including dependencies.
- **tasks**: The main tasks that the role performs.
- **templates**: Jinja2 templates for dynamic file generation.
- **tests**: Directory for role testing.
- **vars**: Variables specific to the role.
- **README.md**: Documentation for the role.


=================================================
ansible-package rolefile.yaml

- hosts: my_hosts
  roles:
    - my_role

=================================================

#To create a new role
ansible-galaxy init <author>.<role_name>

#Pull from ansible repo repo
ansible-galaxy install <author>.<role_name>

#Run the Playbook with the Role
ansible-playbook my_playbook.yml

=================================================


======================================================================================================================

TERRAFORM

AWS CLOUD FORMATION / ARM / GRM

https://developer.hashicorp.com/terraform/downloads




provider "aws" {
  region     = "us-east-1"
  access_key = "AKIAQ654CTC6IIGWXLGS"
  secret_key = "VArx7gmAi49TooMT"
}

# Create AWS Instance

resource "aws_instance" "NewInstance" {
  
  ami           = "ami-0261755bbcb8c4a84"
  instance_type = "t2.micro"
  key_name      = "ClassNewKeyPair"

  tags = {
    Name = "TerraDemo"
  }
}


-----------------------------
1. Amazon Virtual Private Cloud (VPC): VPC is the fundamental networking construct in AWS. 
It allows you to create an isolated network within the AWS cloud, providing you with control 
over IP address ranges, subnets, routing, and security.

2. Subnet: Subnets are subdivisions of a VPC and exist within an Availability Zone (AZ). 
You can create public and private subnets. Public subnets have a route to the internet, while private subnets do not.

4. Security Groups: Security Groups act as a virtual firewall for your EC2 instances. 
You can specify inbound and outbound traffic rules to control access to your instances.

7. Internet Gateway: An Internet Gateway allows traffic from your VPC to the public internet and vice versa. 
It's essential for instances in public subnets to access the internet.

-----------------------------



AKIAQ654CTC6IIGWXLGS
VArx7gmAi49TooMTTXm3y1EhEjVcr4zXxg6sJZrI

======================================================================================================================

PROJECT

www.google.com --> Host based routing
www.google.com/abc -sdjnudsdsjd

Terroform 
	- Provision the servers for complete process.
Ansible 
	- Install required tool On the servers
GitHub
	- Source code repository to maintain to code change.
Jenkins
	- Configure the Master(Controller)
	- Multiple required nodes(Build servers) For the maven Build.
	- Build Triggers
Docker 
	- Docker image Build and push.
Kubernetes
	- ssh publisher to copy the file from the Build server to kubernetes Master.
	- Using the deployment file
	- kubectl create -f nginx-deploy.yaml 
------------------
Integrate Prometheus and Grafana for the all servers and Kubernetes cluster







